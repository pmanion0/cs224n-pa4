\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format

\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{subfig}
\usepackage{multicol}
\usepackage{listings}
\usepackage{caption}
\usepackage{lipsum}


%SetFonts

%SetFonts


\title{CS224N PA4: Neural Networks for Named Entity Recognition}
\author{
	Daoying Lin (SUID 06090664)\\
	\and
	Patrick Manion (SUID )
}
\date{}							% Activate to display a given date or no date


\begin{document}
\maketitle

\section{System Design}
\subsection{Overview of System Implementation Details}
\subsection{Design Choices}




\section{Gradient}
It can be shown that the expression for $\frac{\partial J(\theta)}{\partial L}$ is:
\[
\frac{\partial J(\theta)}{\partial L}
= W^T U^T (p_{\theta} - y) \odot tanh'(Wx + b^{(1)})  \\
\].

We also generalized the gradient expression for multiple layers of neural network, which is summarized next. 

\subsection{Gradient Derivation (With Extra Credit)}

Let $w_{jk}^l$ denote the weight for connecting the $k^{th}$ neuron in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer; $b_j^l$ denote the bias for the $j^{th}$ neuron in the $l^{th}$ layer; $a_j^l$ denote the activation of the $j^{th}$ neuron in the $l^{th}$ layer; $z_j^l$ denote the weighted input to the $j^{th}$ neuron in the $l^{th}$ layer; $h_l(.)$ denote the activation function for the weighted input $\bold{z}_l$.  Note that $z_j^l = \sum_{i} w_{ji}^l a_i^{l-1} + b_j^l$ and $a_j^l = h_l(z_j^l)$. Let's define $\delta_j^l = \frac{\partial J}{\partial z_j^l}$, the error of neuron $j$ in layer $l$. Then it can be easily derived that the following four equations are true for any backpropagation system with any number of hidden layers:
\begin{subequations}
\begin{align}
& \delta^L = \frac{\partial J}{ \partial a^L} \odot h_L'(z^L) \\
& \delta^l  = (W^{l+1})^T \delta^{l+1} \odot h_l'(z^l)   \text{   for $l = 1, ..., L-1$} \\
& \frac{\partial J}{ \partial b_j^l}   = \delta_j^l  \text{   for $l = 1, ..., L$}  \\
& \frac{\partial J}{\partial w^l_{jk}} = \delta_j^l  (a_k^{l-1})^T  \text{   for $l = 1, ..., L$} 
\end{align}
\end{subequations}
where $h_L'(z^L) = p_{\theta} * (1 - p_{\theta})$ and $tanh'(x) = 1 - tanh^2(x)$.


For current system, we've three layers: input layer, hidden layer and output layer. The cost function is $J = - y lna^L$. Using the above general system, we can obtain the following:
\begin{subequations}
\begin{align}
& \delta^3 =  p_{\theta} - y\\
& \delta^2 = (W^3)^T \delta^3 \odot tanh'(z^2) = U^T (p_{\theta} - y)  \odot tanh'(Wx + b^{(1)}) \\
& \delta^1 = (W^2)^T \delta^2  \odot I'(x)= W^T \delta^2 = W^T U^T \delta^3 \odot tanh'(Wx + b^{(1)})
\end{align}
\end{subequations}
And
\begin{subequations}
\begin{align}
\frac{\partial J}{ \partial U} = \delta^3 (a^2 )^T = tanh(Wx + b^{(1)}) (p_{\theta} - y) \\
\frac{\partial J}{ \partial W} = \delta^2 (a^1 )^T = LU^T(p_{\theta} - y) \odot tanh'(Wx + b^{(1)}) \\
\frac{\partial J}{ \partial L} =  \delta^1 = W^T U^T (p_{\theta} - y) \odot tanh'(Wx + b^{(1)})  \\
\frac{\partial J}{ \partial b^{(2)}} = \delta^3 = p_{\theta} - y \\
\frac{\partial J}{ \partial b^{(1)}} = \delta^2 = U^T(p_{\theta} - y) \odot tanh'(Wx + b^{(1)}) 
\end{align}
\end{subequations}



\subsection{Gradient Check Results}
We implemented gradient check and passed the test. 
\\TODO: attach the output maybe? 


\section{Parameter Tuning}
We ran a number of tests to compare the performance of the neural networks. After tuning we found good performance with this benchmark (describe), which achieves 82.42 F1 score on test. To explore the effect of other parameters individually, we use this set of parameters as our base line and vary the parameter we'd like to investigate on. The following are the findings. 

\subsection{Learning Rate $\alpha$}
\subsection{Max Iterations $I$}
\subsection{Regularization Value $\lambda$}
\subsection{Hidden Layer Size $H$}
We trained a series of neural networks with single hidden layer while we varied the size of hidden layer which ranges from 100 to 300 with step size of 50. From figure? we observed that the performance is very flat for both training and test set. For single layer neural network, 100 is a good value to choose. 

\subsection{Window Size $C$}
To study the effect of window size, we varied the value of window from 3 to 15 with step size 2. From figure? We observed there is a substantial performance improvement from 3 to 5. After that, F1 score increases as the window size increases on training. However, F1 score remains relatively flat after 5 on test. This suggests that a window size 5 is a good value to choose and there is probably overfitting on training when window size is greater than 5. 

\subsection{Fixed vs. Updated Word Vectors}

\subsection{Randomly Initialized vs. Pre-trained Word Vectors}
We also compared the performance of using randomly initialized word vectors and pre-trained word vectors. Based on figure?, pre-training improved the performance dramatically on both training and test data. Does pre-training always help? Intuitively, if words were pertained on relevant corpus this should be the case. But what if words were pre-trained on Wall Street Journal and the problem is to do NER on corpus related to Football or some other not relevant field? Without more experiments we can't make a general conclusion. 

\section{Error Analysis}

We analyzed the error by entity. For each entity, we reported two misclassification errors: False Positive (FP) and False Negative (FN). Take PER as an example,  FP means misclassification of non-PER word as PER and FN means misclassification of PER as any non-PER word. Here non-PER word includes LOC, ORG, MISC and O. 


\subsection{PER}
\begin{itemize}
\item \emph{False Positive}
There are 129 cases where O is misclassified as PER. Among them, mostly are numbers (1, 2, 3, 53.98, 1,627, 1988, etc ) and compound adjectives (ex-rebel, Lieutenant-Colonel, newly-signed, over-allotment, soft-spoken, etc.). These misclassification can be avoided if we could enforce some rules. For example, usually numbers won't be a person.

There are 69 cases where ORG is misclassified as PER. Some examples are: Fed, Duke, Ford, Johnson, Kent, Jones, Lola, Magna. A lot of them are actually organization's that are named after their founders. These would be really hard for the algorithm to distinguish since those words could be either cases. 


There are 38 cases where LOC is misclassified as PER. PATRICK. 

There are 25 cases where MISC is misclassified as PER. One very interesting example is the words "Michael" and "Collins". These two are actually human names. The reason the algorithm misclassified them is because there is a movie named "Michael Collins". For cases like this, it's going to be really hard to make the right prediction. 


\item \emph{False Negative}
The false negative is dominated by misclassifying PER as O, which are mostly non-standard English name. For example: Hondo, Inzamam-ul-Haq, Capelli, Djorkaeff, Hun, Wang, Donghai, Xiao, Sihanouk, etc. This type of error can be reduced by pre-training words on more general text that contains non-standard English name.

\end{itemize}

\subsection{MISC}

\begin{itemize}
\item \emph{False Positive}
The false positive is dominated by misclassification of O as MISC. Again, there are a lot of numbers (13, 40, 1988, etc)  and compound adjectives (little-known, 88-year-old, army-backed, etc). 

\item \emph{False Negative} 
The false negative is dominated by misclassifying MISC as O. A lot of them are related to sports event like "ENGLISH COUNTY CHAMPIONSHIP", "U.S. Open Tennis Championship",  "U.S. Amateur Championship", etc.  While the algorithm often correctly classified other words (like U.S. Open) as MISC but failed to classify championship correctly. This is a challenging problem since most of the time "championship" should be O. It's only referring to an entity when in phrase like this. Similarly, MISC is misclassified as LOC because usually those words are indeed location but it's an NER when appears in phrase like "the HONE KONG Open", "the Chicago PMI", "the Berlin Grand Prix", etc. These kinds of error may be reduced by sequence modeling or treating those phrases as if one word. 

\end{itemize}




\section{Extra Credit}
\subsection{Alternative Prepared Word Vectors}


\subsection{Deeper Neural Networks}
Our design and implementation works for any hidden layer architectures. To study the effect of deeper neural networks, we run experiments with two hidden layers and set the size to be the Cartesian combination of $\{50, 100\}$ and $\{50, 150, 300\}$ for the first and second hidden layer, respectively. We also included the performance of baseline model that only has one hidden layer. From ? we can see deeper neural networks indeed improved the performance but only slightly. However, for the same depth, the performance seems not very sensitive to the size of hidden layers. We also implemented 3 and 4 hidden layers of neural networks. Surprisingly, the performance didn't improve as the network gets deeper. More experiments are needed to fully understand the role of the depth of neural networks.

\end{document}  





