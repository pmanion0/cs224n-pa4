\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format

\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{subfig}
\usepackage{multicol}
\usepackage{listings}
\usepackage{caption}
\usepackage{lipsum}


%SetFonts

%SetFonts


\title{CS224N PA4: Neural Networks for Named Entity Recognition}
\author{
	Daoying Lin (SUID 06090664)\\
	\and
	Patrick Manion (SUID )
}
\date{}							% Activate to display a given date or no date


\begin{document}
\maketitle

\section{Gradients for Backpropagation}
Let $w_{jk}^l$ denote the weight for connecting the $k^{th}$ neuron in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer; $b_j^l$ denote the bias for the $j^{th}$ neuron in the $l^{th}$ layer; $a_j^l$ denote the activation of the $j^{th}$ neuron in the $l^{th}$ layer; $z_j^l$ denote the weighted input to the $j^{th}$ neuron in the $l^{th}$ layer; $h_l(.)$ denote the activation function for the weighted input $\bold{z}_l$.  Note that $z_j^l = \sum_{i} w_{ji}^l a_i^{l-1} + b_j^l$ and $a_j^l = h_l(z_j^l)$. Let's define $\delta_j^l = \frac{\partial J}{\partial z_j^l}$, the error of neuron $j$ in layer $l$. Then it can be easily derived that the following four equations are true for any backpropagation system:
\begin{subequations}
\begin{align}
& \delta^L = \frac{\partial J}{ \partial a^L} \odot h_L'(z^L) \\
& \delta^l  = (W^{l+1})^T \delta^{l+1} \odot h_l'(z^l) \\
& \frac{\partial J}{ \partial b_j^l}   = \delta_j^l \\
& \frac{\partial J}{\partial w^l_{jk}} = a_k^{l-1} \delta_j^l 
\end{align}
\end{subequations}

For current system, we've three layers: input layer, hidden layer and output layer. The cost function is $J = - [y lna^L + (1-y) ln(1- a^L)]$. Using the above general system, we can obtain the following:
\begin{subequations}
\begin{align}
& \delta^3 =  p_{\theta} - y\\
& \delta^2 = (W^3)^T \delta^3 \odot tanh'(z^2) = U^T (p_{\theta} - y)  \odot tanh'(Wx + b^{(1)}) \\
& \delta^1 = (W^2)^T \delta^2  \odot I'(x)= W^T \delta^2 = W^T U^T \delta^3 \odot tanh'(Wx + b^{(1)})
\end{align}
\end{subequations}
And
\begin{subequations}
\begin{align}
\frac{\partial J}{ \partial U} = a^2 \delta^3 = tanh(Wx + b^{(1)}) (p_{\theta} - y) \\
\frac{\partial J}{ \partial W} = a^1 \delta^2 = LU^T(p_{\theta} - y) \odot tanh'(Wx + b^{(1)}) \\
\frac{\partial J}{ \partial L} = a^0 \delta^1 = W^T U^T (p_{\theta} - y) \odot tanh'(Wx + b^{(1)})  \\
\frac{\partial J}{ \partial b^{(2)}} = \delta^3 = p_{\theta} - y \\
\frac{\partial J}{ \partial b^{(1)}} = \delta^2 = U^T(p_{\theta} - y) \odot tanh'(Wx + b^{(1)}) 
\end{align}
\end{subequations}

\end{document}  